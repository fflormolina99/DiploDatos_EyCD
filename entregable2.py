# -*- coding: utf-8 -*-
"""entregable_parte2_ejerc1_al_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l8Db3eOrnZOBDNX2c3e7edApNOW-xV28
"""

import matplotlib.pyplot as plt
import numpy
import pandas

import seaborn

seaborn.set_context("talk")

from google.colab import files
import io

uploaded = files.upload()

file_key = "melb_data_extended.csv"  # Replace for correspoing key
melb_df = pandas.read_csv(io.StringIO(uploaded[file_key].decode("utf-8")))

"""
Ejercicio 1: Encoding
"""

# me quedo con el nombre de las columnas que luego usaré para dictvectorizer
melb_df_columns = melb_df.columns.to_list()
# melb_df_columns

# saco las dos columnas que no deben ser consideradas
feature_cols = []
for col in melb_df_columns:
    if col == "BuildingArea" or col == "YearBuilt":
        continue
    else:
        feature_cols.append(col)
# print(feature_cols)

# convierto todo en un diccionario
feature_dict = list(melb_df[feature_cols].T.to_dict().values())
# feature_dict[:2]

# aplico dictvectorizer
from sklearn.feature_extraction import DictVectorizer

vec = DictVectorizer()
feature_matrix = vec.fit_transform(feature_dict)

# Fill nan values on sparse matrix.
if numpy.any(numpy.isnan(feature_matrix.data)):
    feature_matrix.data = numpy.nan_to_num(feature_matrix.data)

# convierto la matriz esparsa en densa
precision_type = numpy.float32
dense_feature_matrix = feature_matrix.astype(precision_type).todense()

# transformo la matriz densa en un dataframe
melb_df_encoded2 = pandas.DataFrame(dense_feature_matrix)
melb_df_encoded2.columns = vec.get_feature_names_out()
# melb_df_encoded2[:3]

"""
Ejercicio 2: Imputación por KNN
"""

# adiciono las dos columnas que saque antes al nuevo dataframe
melb_df_encoded2["YearBuilt"] = melb_df["YearBuilt"]
melb_df_encoded2["BuildingArea"] = melb_df["BuildingArea"]

# se aplica imputación a la matriz transformada en df en las dos nuevas columnas
from sklearn.experimental import enable_iterative_imputer
from sklearn.neighbors import KNeighborsRegressor
from sklearn.impute import IterativeImputer

melb_data_mice = melb_df_encoded2.copy(deep=True)

mice_imputer = IterativeImputer(random_state=0, estimator=KNeighborsRegressor())
melb_data_mice[["YearBuilt", "BuildingArea"]] = mice_imputer.fit_transform(
    melb_data_mice[["YearBuilt", "BuildingArea"]]
)

# se grafican las diferencias antes y luego de la imputación
mice_year_built = melb_data_mice.YearBuilt.to_frame()
mice_year_built["Imputation"] = "KNN over YearBuilt and BuildingArea"
melb_year_build = melb_df.YearBuilt.dropna().to_frame()
melb_year_build["Imputation"] = "Original"
data = pandas.concat([mice_year_built, melb_year_build]).reset_index()
fig = plt.figure(figsize=(8, 5))
g = seaborn.kdeplot(data=data, x="YearBuilt", hue="Imputation")

melb_data_mice.values

"""
Ejercicio 3: Reducción de dimensionalidad.
"""

# se esalan los datos antes de aplicar PCA
# TODO Here we have to pre-process the matrix before applying PCA
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(
    feature_range=(-1, 1)
)  # aqui se está aplicando estandarización/escalación
scaled_dense_feature_matrix = scaler.fit_transform(melb_data_mice)
numpy.max(scaled_dense_feature_matrix), numpy.min(scaled_dense_feature_matrix)

n = min(20, scaled_dense_feature_matrix.shape[0])

# se aplica PCA
from sklearn.decomposition import PCA

n = n
pca = PCA(n_components=n)
pca.fit(scaled_dense_feature_matrix)
proyected_features = pca.transform(scaled_dense_feature_matrix)
proyected_features.shape

print("Principal components")
print(pca.components_)
print("Explained variance ratio")
print(pca.explained_variance_ratio_)

# se grafica la varianza de cada PC
PC_values = numpy.arange(pca.n_components_) + 1
plt.plot(PC_values, pca.explained_variance_ratio_, "o-", linewidth=2, color="blue")
plt.title("Scree Plot")
plt.xlabel("Principal Component")
plt.ylabel("Variance Explained")
plt.show()

pca_df = pandas.DataFrame(
    data=proyected_features,
    columns=[
        "PC 1",
        "PC 2",
        "PC 3",
        "PC 4",
        "PC 5",
        "PC 6",
        "PC 7",
        "PC 8",
        "PC 9",
        "PC 10",
        "PC 11",
        "PC 12",
        "PC 13",
        "PC 14",
        "PC 15",
        "PC 16",
        "PC 17",
        "PC 18",
        "PC 19",
        "PC 20",
    ],
)
print(pca_df)

"""
Ejercicio 4: Composición del resultado
"""

# se concatenan los 5 primeros PC con el dataframe
processed_melb_df = pandas.concat([melb_data_mice, pca_df.iloc[:, :6]], axis=1)
processed_melb_df.head()
